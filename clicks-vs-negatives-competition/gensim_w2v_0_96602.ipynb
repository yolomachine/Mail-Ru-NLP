{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import  random\n",
    "from tqdm.notebook import tqdm\n",
    "from functools import reduce\n",
    "from gensim.models import Word2Vec\n",
    "from scipy import spatial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "class DataFrameDesc:\n",
    "    def __init__(self, filepath_or_buffer, sep, usecols, names, chunksize, total):\n",
    "        self.filepath_or_buffer = filepath_or_buffer\n",
    "        self.sep = sep\n",
    "        self.usecols = usecols\n",
    "        self.names = names\n",
    "        self.chunksize = chunksize\n",
    "        self.total = total\n",
    "\n",
    "\n",
    "class DataFrameIterable:\n",
    "    def __init__(self, descriptor):\n",
    "        self.descriptor = descriptor\n",
    "\n",
    "    def __iter__(self):\n",
    "        print(f'Reading \"{self.descriptor.filepath_or_buffer}\"')\n",
    "        print(f'Total: {self.descriptor.total}')\n",
    "        print(f'Chunk size: {self.descriptor.chunksize}')\n",
    "        for _, df in tqdm(iterable=enumerate(pd.read_csv(filepath_or_buffer=self.descriptor.filepath_or_buffer,\n",
    "                                                         sep=self.descriptor.sep,\n",
    "                                                         usecols=self.descriptor.usecols,\n",
    "                                                         names=self.descriptor.names,\n",
    "                                                         chunksize=self.descriptor.chunksize)),\n",
    "                          total=self.descriptor.total // self.descriptor.chunksize):\n",
    "            df = df.fillna('')\n",
    "            for _, row in df.iterrows():\n",
    "                yield tuple([row[name] for name in self.descriptor.names])\n",
    "\n",
    "\n",
    "class CompoundDataFrameIterable:\n",
    "    def __init__(self, iterables):\n",
    "        self.iterables = iterables\n",
    "\n",
    "    def __iter__(self):\n",
    "        for iterable in self.iterables:\n",
    "            yield from iterable\n",
    "\n",
    "\n",
    "\n",
    "class CorpusIterable:\n",
    "    def __init__(self, string_list_iter):\n",
    "        self.string_list_iter = string_list_iter\n",
    "\n",
    "    def __iter__(self):\n",
    "        for string_list_data in self.string_list_iter:\n",
    "            yield reduce(lambda x, y: x + y.split(), string_list_data[1:], [])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "chunksize = 100000\n",
    "train_df_desc = DataFrameDesc('clicks_10M', '\\t', [0, 1, 2], ['_', 'q', 'r'], chunksize, total=int(1e7))\n",
    "test_df_desc  = DataFrameDesc('clicks_test_', '\\t', [0, 1, 2], ['_', 'q', 'r'], chunksize, total=int(1e6 // 2))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading \"clicks_10M\"\n",
      "Total: 10000000\n",
      "Chunk size: 100000\n",
      "Reading \"clicks_test_\"\n",
      "Total: 500000\n",
      "Chunk size: 100000\n",
      "Reading \"clicks_10M\"\n",
      "Total: 10000000\n",
      "Chunk size: 100000\n",
      "Reading \"clicks_test_\"\n",
      "Total: 500000\n",
      "Chunk size: 100000\n",
      "Reading \"clicks_10M\"\n",
      "Total: 10000000\n",
      "Chunk size: 100000\n",
      "Reading \"clicks_test_\"\n",
      "Total: 500000\n",
      "Chunk size: 100000\n",
      "Reading \"clicks_10M\"\n",
      "Total: 10000000\n",
      "Chunk size: 100000\n",
      "Reading \"clicks_test_\"\n",
      "Total: 500000\n",
      "Chunk size: 100000\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/100 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "467638d878564faebafb6cd12146e7c2"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/5 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1fddc1ca833e41a8901a63943bb29b74"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/100 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "fae11d48597248498a440b9dfbe6beb4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/5 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6272829a5a674f73ba2aa1574f520a4e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/100 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2ecd5395bae0496c90095e3a0f9d8771"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/5 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1d44266e5b0748728a0a37ae583a3b7c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/100 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "27eda5566b454501ae8cca166108d5b9"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/5 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8eb392746ac44f50b87f0799fa89de6c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "(390192994, 417966930)"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Word2Vec(min_count=10, sample=1e-3)\n",
    "model.build_vocab(corpus_iterable=CorpusIterable(\n",
    "    CompoundDataFrameIterable([\n",
    "        DataFrameIterable(train_df_desc),\n",
    "        DataFrameIterable(test_df_desc)\n",
    "    ])\n",
    "))\n",
    "model.train(corpus_iterable=CorpusIterable(\n",
    "    CompoundDataFrameIterable([\n",
    "        DataFrameIterable(train_df_desc),\n",
    "        DataFrameIterable(test_df_desc)\n",
    "    ])\n",
    "), total_examples=model.corpus_count, epochs=3)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading \"clicks_test_\"\n",
      "Total: 500000\n",
      "Chunk size: 100000\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/5 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "03b8b6a0ac1644debf69167d914ed28b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "scores = []\n",
    "for data in DataFrameIterable(test_df_desc):\n",
    "    query = list(filter(lambda x: x in model.wv.key_to_index, data[1].split()))\n",
    "    title = list(filter(lambda x: x in model.wv.key_to_index, data[2].split()))\n",
    "    if len(query) == 0 or len(title) == 0:\n",
    "        scores.append(random.uniform(0, 1))\n",
    "        continue\n",
    "    v1 = sum(model.wv[x] for x in query) / len(query)\n",
    "    v2 = sum(model.wv[x] for x in title) / len(title)\n",
    "    scores.append(1 - spatial.distance.cosine(v1, v2) / 2.0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "with open('res_1.csv', 'w') as f:\n",
    "    f.write('Id,Predicted\\n')\n",
    "    for i, p in enumerate(scores):\n",
    "        f.write(f'{i},{p:0.6f}\\n')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}